首次启动Hadoop
=======================
    1. 格式化文件系统
       hadoop namenode -format

    2. 启动所有进程
       start-all.sh
    3. 查询进程
    4. 停止所有进程 
       stop-all.sh

使用webui访问hadoop hdfs
=========================
    1. hdfs webui http://ip:50070

    2. Datanode ip:50075

    3. 2nn ip:50090

完全分布式
=========================
    1. 准备多台机器
    2. 安装jdk并配置环境变量
       JAVA_HOME PATH
    3. 安装hadoop并配置环境变量
       HADOOP_HOME PATH
    4. 安装SSH
       把NameNode配置成可以无密访问其他机器

远程同步
=========================
scp 
rsync 元数据拷贝很强大   




在所有机器上执行相同的命令
=============================
xcall
------------------
#!/bin/bash

if (( $#<1 )) ; then
    echo no args;
    exit;
fi

for (( host=200 ; host<205 ;  host=host+1 )) ; do
    echo -------------s$host---------------
    ssh s$host source /etc/profile && $@
done

xsync
------------------
#!/bin/bash

if (( $#<1 )) ; then
    echo no args;
    exit;
fi


fname=`basename $1`

pdir=`cd -P $(dirname $1) ;  pwd`

for (( host=200 ;  host<205 ; host=host+1 )) ; do
    echo -----------s$host-------------
    rsync -rl $pdir/$fname `whoami`@s$host:$pdir
done


完全分布式集群配置
=============================
1. 准备5台客户机
2. 安装jdk配置环境变量
3. 安装hadoop配置环境变量
4. 安装ssh配置无密登录
5. 配置文件
   [$HADOOP_HOME/etc/hadoop/core-site.xml]
   fs.defaultFS=hdfs://s200/

   [$HADOOP_HOME/etc/hadoop/yarn-site.xml]
   yarn.resourcemanager.hostname=s200

   [$HADOOP_HOME/etc/hadoop/hdfs-site.xml]
   dfs.replication=3

   [$HADOOP_HOME/etc/hadoop/slaves]
   s201
   s202
   s203
6. 安装rsync
7. 在集群上分发配置文件



hadoop fs -put hello.txt /user/ubuntu/data/

1. /etc/sysconfig/network
2. /etc/sysconfig/network-scripts/ifcfg-e...  
   /etc/resolv.conf
   /etc/hosts
3. service firewalld stop
4. chkconfig firewalld off
5. yum install -y vim openssh-server net-tools.x86_64 rsync.x86_64
6. ~/.bashrc alias vi='vim'
7. JAVA_HOME HADOOP_HOME PATH hadoop-env.sh

本地文件的内容
===============================
    1. 名称节点s200: /tmp/hadoop-root/dfs/name/current/VERSION 
	  #Fri Apr 07 21:07:18 EDT 2017
      namespaceID=1824721895
      clusterID=CID-d00f3274-4f56-46a2-89b1-ce6d01200214
      cTime=0
      storageType=NAME_NODE
      blockpoolID=BP-1260364991-192.168.0.200-1491613638266
      layoutVersion=-63
	  
	  数据节点s201: /tmp/hadoop-root/dfs/data/current/VERSION 
      #Fri Apr 07 21:19:35 EDT 2017
      storageID=DS-9308d42d-db01-4638-bc13-d16a54b254c7
      clusterID=CID-d00f3274-4f56-46a2-89b1-ce6d01200214
      cTime=0
      datanodeUuid=51f700b0-1921-4301-9beb-dabff17469b6
      storageType=DATA_NODE
      layoutVersion=-56

	  数据节点s202:
	  #Fri Apr 07 21:19:35 EDT 2017
      storageID=DS-ae31ac67-f4d2-4766-b1c6-b1e9b9d57415
      clusterID=CID-d00f3274-4f56-46a2-89b1-ce6d01200214
      cTime=0
      datanodeUuid=f88543a8-49a6-480e-9645-225c253d3d8b
      storageType=DATA_NODE
      layoutVersion=-56
  
      数据节点s203: 略
	  
	  辅助名称节点:
	  cat /tmp/hadoop-root/dfs/namesecondary/current/VERSION
      #Fri Apr 07 23:16:07 EDT 2017
      namespaceID=1824721895
      clusterID=CID-d00f3274-4f56-46a2-89b1-ce6d01200214
      cTime=0
      storageType=NAME_NODE
      blockpoolID=BP-1260364991-192.168.0.200-1491613638266
      layoutVersion=-63

      
修改本地的临时目录
===============================
    1. [core-site.xml]
	   hadoop.tmp.dir=/home/ubuntu/hadoop
	2. 分发配置
	3. stop-all.sh
	4. hadoop namenode -format
	5. start-all.sh

针对单个主机的进程操作
===============================
hadoop-daemon.sh start/stop datanode/namenode/secondarynamenode
hadoop-daemons.sh ....
	
	
HDFS常用操作
================================
	1. 格式化 hadoop fs -format
	2. hadoop fs == hdfs dfs
	3. hadoop fs -put  上传到hdfs文件系统
	             -copyFromLocal
	4. hdfs dfs -copyToLocal
	            -get          
	5. hdfs dfs -moveFromLocal/-moveToLocal
	6. hdfs dfs -rm -r 
	7. hdfs dfs -cp 在HDFS上进行拷贝
	
hadoop hdfs文件块大小128M
=================================
    1. 减少磁盘寻到时间 
	
	
通过API访问HDFS
=================================
    1. eclipse 开发环境
	2. 导入jar包
	3. URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
	4. URL u = new URL("hdfs://...");
	   URLConnection conn = u.openConnection();
	   InputStream is = conn.getInputStream();
	5. FileSystem Api 
	   
	
	
